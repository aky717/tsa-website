Topic_Number,Keywords,Abstract_1,Abstract_2,Abstract_3,Abstract_4,Abstract_5,Abstract_6,Abstract_7,Abstract_8,Abstract_9,Abstract_10,Perc_of_Corpus,Topic Cluster,Summary topic
1,metal; intak; oxid; studi; cell; antioxid; bodi; digest; associ; ros; can; oxid_stress; heavi; metabol; activ,"Objective To summarise evidence on the association between intake of dietary sugars and body weight in adults and children. Design Systematic review and meta-analysis of randomised controlled trials and prospective cohort studies. Data sources OVID Medline, Embase, PubMed, Cumulative Index to Nursing and Allied Health Literature, Scopus, and Web of Science (up to December 2011). Review methods Eligible studies reported the intake of total sugars, intake of a component of total sugars, or intake of sugar containing foods or beverages; and at least one measure of body fatness. Minimum duration was two weeks for trials and one year for cohort studies. Trials of weight loss or confounded by additional medical or lifestyle interventions were excluded. Study selection, assessment, validity, data extraction, and analysis were undertaken as specified by the Cochrane Collaboration and the GRADE working group. For trials, we pooled data for weight change using inverse variance models with random effects. We pooled cohort study data where possible to estimate effect sizes, expressed as odds ratios for risk of obesity or β coefficients for change in adiposity per unit of intake. Results 30 of 7895 trials and 38 of 9445 cohort studies were eligible. In trials of adults with ad libitum diets (that is, with no strict control of food intake), reduced intake of dietary sugars was associated with a decrease in body weight (0.80 kg, 95% confidence interval 0.39 to 1.21; P<0.001); increased sugars intake was associated with a comparable weight increase (0.75 kg, 0.30 to 1.19; P=0.001). Isoenergetic exchange of dietary sugars with other carbohydrates showed no change in body weight (0.04 kg, −0.04 to 0.13). Trials in children, which involved recommendations to reduce intake of sugar sweetened foods and beverages, had low participant compliance to dietary advice; these trials showed no overall change in body weight. However, in relation to intakes of sugar sweetened beverages after one year follow-up in prospective studies, the odds ratio for being overweight or obese increased was 1.55 (1.32 to 1.82) among groups with the highest intake compared with those with the lowest intake. Despite significant heterogeneity in one meta-analysis and potential bias in some trials, sensitivity analyses showed that the trends were consistent and associations remained after these studies were excluded. Conclusions Among free living people involving ad libitum diets, intake of free sugars or sugar sweetened beverages is a determinant of body weight. The change in body fatness that occurs with modifying intakes seems to be mediated via changes in energy intakes, since isoenergetic exchange of sugars with other carbohydrates was not associated with weight change.","Objectives: Red and processed meat consumption is adversely related to cardiometabolic risk, but the impact of overall dietary quality on this association has not been systematically investigated. We examined the influence of dietary quality on associations of meat intake with biomarkers of cardiometabolic risk. Methods:Data are from the JacksonHeart Study, a cohort of African Americans (baseline age 55 y, 66% female, 20% diabetes, 9% CVD). We analyzed those with biomarker data available at Visit 1 (2000– 04) and at Visit 2 (2005–09) or 3 (2009–13). Diet was assessed by food frequency questionnaire (Visit 1). Total observations used were: Visit 1 (n = 3725), Visit 2 (n = 2736), and Visit 3 (n = 3319). Unprocessed red meat included beef and pork, and processed meat included sausage, lunch, and cured meats. Diet quality was measured by a modified Healthy Eating Index 2010 score (m-HEI) that excluded meat contributions. Modified HEI stratified and unstratified analyses were conducted using linear mixed modeling. Fasting HbA1c and CRP values were log transformed. Results: Meat consumption was not associated with HbA1c in mHEI stratified or unstratified analyses. A 1 oz/1000 kcal/wk increase in unprocessed red and total meat was associated with a 1.3% ± 0.5% (P = 0.02) and 1.1% ± 0.3% (P = 0.005) higher CRP in unstratified analyses, respectively. Unprocessed red meat was positively associated with CRP in m-HEI tertiles 1 (2.0% ± 0.8%, P = 0.01) and 3 (2.2%± 0.8%,P= 0.008). Totalmeatwas associatedwithCRP inm-HEI tertile 1 (2.0%± 0.6%, P= 0.001) and trended in tertile 3 (1.1%± 0.6%, P= 0.09); processedmeat also approached significance inm-HEI tertile 1 (2.1%± 1.2%, P= 0.08). There was evidence thatm-HEImodified the associations between processed meat and CRP (P-interaction = 0.04), but not for other associations. Excluding those with diabetes or CVD did not alter these results. Conclusions: Our results do not support that meat intake is associated with HbA1c, or that overall dietary quality modifies these associations. Unprocessed red and total meat intakes were associated with greater CRP in unstratified and subsets of stratified analyses. Associations of processed meat with CRP appeared stronger among those with the poorest diet quality. These data suggest that reduction in red meat intake could benefit inflammation among African American adults. Funding Sources: The Beef Checkoff.","Reactive oxygen species (ROS) constitute a group of highly reactive molecules that have evolved as regulators of important signaling pathways. It is now well accepted that moderate levels of ROS are required for several cellular functions, including gene expression. The production of ROS is elevated in tumor cells as a consequence of increased metabolic rate, gene mutation and relative hypoxia, and excess ROS are quenched by increased antioxidant enzymatic and nonenzymatic pathways in the same cells. Moderate increases of ROS contribute to several pathologic conditions, among which are tumor promotion and progression, as they are involved in different signaling pathways and induce DNA mutation. However, ROS are also able to trigger programmed cell death (PCD). Our review will emphasize the molecular mechanisms useful for the development of therapeutic strategies that are based on modulating ROS levels to treat cancer. Specifically, we will report on the growing data that highlight the role of ROS generated by different metabolic pathways as Trojan horses to eliminate cancer cells. Highly reactive molecules called reactive oxygen species (ROS), which at low levels are natural regulators of important signaling pathways in cells, might be recruited to act as “Trojan horses” to kill cancer cells. Researchers in Italy led by Bruno Perillo of the Institute of Food Sciences in Avelllino review the growing evidence suggesting that stimulating production of natural ROS species could become useful in treating cancer. Although ROS production is elevated in cancer cells it can also promote a natural process called programmed cell death. This normally regulates cell turnover, but could be selectively activated to target diseased cells. The authors discuss molecular mechanisms underlying the potential anti-cancer activity of various ROS-producing strategies, including drugs and light-stimulated therapies. They expect modifying the production of ROS to have potential for developing new treatments.","Background: Public health recommendations and governmental measures during the COVID-19 pandemic have resulted in numerous restrictions on daily living including social distancing, isolation and home confinement. While these measures are imperative to abate the spreading of COVID-19, the impact of these restrictions on health behaviours and lifestyles at home is undefined. Therefore, an international online survey was launched in April 2020, in seven languages, to elucidate the behavioural and lifestyle consequences of COVID-19 restrictions. This report presents the results from the first thousand responders on physical activity (PA) and nutrition behaviours. Methods: Following a structured review of the literature, the “Effects of home Confinement on multiple Lifestyle Behaviours during the COVID-19 outbreak (ECLB-COVID19)” Electronic survey was designed by a steering group of multidisciplinary scientists and academics. The survey was uploaded and shared on the Google online survey platform. Thirty-five research organisations from Europe, North-Africa, Western Asia and the Americas promoted the survey in English, German, French, Arabic, Spanish, Portuguese and Slovenian languages. Questions were presented in a differential format, with questions related to responses “before” and “during” confinement conditions. Results: 1047 replies (54% women) from Asia (36%), Africa (40%), Europe (21%) and other (3%) were included in the analysis. The COVID-19 home confinement had a negative effect on all PA intensity levels (vigorous, moderate, walking and overall). Additionally, daily sitting time increased from 5 to 8 h per day. Food consumption and meal patterns (the type of food, eating out of control, snacks between meals, number of main meals) were more unhealthy during confinement, with only alcohol binge drinking decreasing significantly. Conclusion: While isolation is a necessary measure to protect public health, results indicate that it alters physical activity and eating behaviours in a health compromising direction. A more detailed analysis of survey data will allow for a segregation of these responses in different age groups, countries and other subgroups, which will help develop interventions to mitigate the negative lifestyle behaviours that have manifested during the COVID-19 confinement.","Obesity is a chronic disease of multifactorial origin and can be defined as an increase in the accumulation of body fat. Adipose tissue is not only a triglyceride storage organ, but studies have shown the role of white adipose tissue as a producer of certain bioactive substances called adipokines. Among adipokines, we find some inflammatory functions, such as Interleukin-6 (IL-6); other adipokines entail the functions of regulating food intake, therefore exerting a direct effect on weight control. This is the case of leptin, which acts on the limbic system by stimulating dopamine uptake, creating a feeling of fullness. However, these adipokines induce the production of reactive oxygen species (ROS), generating a process known as oxidative stress (OS). Because adipose tissue is the organ that secretes adipokines and these in turn generate ROS, adipose tissue is considered an independent factor for the generation of systemic OS. There are several mechanisms by which obesity produces OS. The first of these is the mitochondrial and peroxisomal oxidation of fatty acids, which can produce ROS in oxidation reactions, while another mechanism is over-consumption of oxygen, which generates free radicals in the mitochondrial respiratory chain that is found coupled with oxidative phosphorylation in mitochondria. Lipid-rich diets are also capable of generating ROS because they can alter oxygen metabolism. Upon the increase of adipose tissue, the activity of antioxidant enzymes such as superoxide dismutase (SOD), catalase (CAT), and glutathione peroxidase (GPx), was found to be significantly diminished. Finally, high ROS production and the decrease in antioxidant capacity leads to various abnormalities, among which we find endothelial dysfunction, which is characterized by a reduction in the bioavailability of vasodilators, particularly nitric oxide (NO), and an increase in endothelium-derived contractile factors, favoring atherosclerotic disease.","In mammals, circadian oscillators exist not only in the suprachiasmatic nucleus, which harbors the central pacemaker, but also in most peripheral tissues. It is believed that the SCN clock entrains the phase of peripheral clocks via chemical cues, such as rhythmically secreted hormones. Here we show that temporal feeding restriction under light-dark or dark-dark conditions can change the phase of circadian gene expression in peripheral cell types by up to 12 h while leaving the phase of cyclic gene expression in the SCN unaffected. Hence, changes in metabolism can lead to an uncoupling of peripheral oscillators from the central pacemaker. Sudden large changes in feeding time, similar to abrupt changes in the photoperiod, reset the phase of rhythmic gene expression gradually and are thus likely to act through a clock-dependent mechanism. Food-induced phase resetting proceeds faster in liver than in kidney, heart, or pancreas, but after 1 wk of daytime feeding, the phases of circadian gene expression are similar in all examined peripheral tissues.","The industrial activities of the last century have caused massive increases in human exposure to heavy metals. Mercury, lead, chromium, cadmium, and arsenic have been the most common heavy metals that induced human poisonings. Here, we reviewed the mechanistic action of these heavy metals according to the available animal and human studies. Acute or chronic poisonings may occur following exposure through water, air, and food. Bioaccumulation of these heavy metals leads to a diversity of toxic effects on a variety of body tissues and organs. Heavy metals disrupt cellular events including growth, proliferation, differentiation, damage-repairing processes, and apoptosis. Comparison of the mechanisms of action reveals similar pathways for these metals to induce toxicity including ROS generation, weakening of the antioxidant defense, enzyme inactivation, and oxidative stress. On the other hand, some of them have selective binding to specific macromolecules. The interaction of lead with aminolevulinic acid dehydratase and ferrochelatase is within this context. Reactions of other heavy metals with certain proteins were discussed as well. Some toxic metals including chromium, cadmium, and arsenic cause genomic instability. Defects in DNA repair following the induction of oxidative stress and DNA damage by the three metals have been considered as the cause of their carcinogenicity. Even with the current knowledge of hazards of heavy metals, the incidence of poisoning remains considerable and requires preventive and effective treatment. The application of chelation therapy for the management of metal poisoning could be another aspect of heavy metals to be reviewed in the future.","Background On December 12th 2019, a new coronavirus (SARS-Cov2) emerged in Wuhan, China, sparking a pandemic of acute respiratory syndrome in humans (COVID-19). On the 24th of April 2020, the number of COVID-19 deaths in the world, according to the COVID-Case Tracker by Johns Hopkins University, was 195,313, and the number of COVID-19 confirmed cases was 2,783,512. The COVID-19 pandemic represents a massive impact on human health, causing sudden lifestyle changes, through social distancing and isolation at home, with social and economic consequences. Optimizing public health during this pandemic requires not only knowledge from the medical and biological sciences, but also of all human sciences related to lifestyle, social and behavioural studies, including dietary habits and lifestyle. Methods Our study aimed to investigate the immediate impact of the COVID-19 pandemic on eating habits and lifestyle changes among the Italian population aged ≥ 12 years. The study comprised a structured questionnaire packet that inquired demographic information (age, gender, place of residence, current employment); anthropometric data (reported weight and height); dietary habits information (adherence to the Mediterranean diet, daily intake of certain foods, food frequency, and number of meals/day); lifestyle habits information (grocery shopping, habit of smoking, sleep quality and physical activity). The survey was conducted from the 5th to the 24th of April 2020. Results A total of 3533 respondents have been included in the study, aged between 12 and 86 years (76.1% females). The perception of weight gain was observed in 48.6% of the population; 3.3% of smokers decided to quit smoking; a slight increased physical activity has been reported, especially for bodyweight training, in 38.3% of respondents; the population group aged 18–30 years resulted in having a higher adherence to the Mediterranean diet when compared to the younger and the elderly population (p < 0.001; p < 0.001, respectively); 15% of respondents turned to farmers or organic, purchasing fruits and vegetables, especially in the North and Center of Italy, where BMI values were lower. Conclusions In this study, we have provided for the first time data on the Italian population lifestyle, eating habits and adherence to the Mediterranean Diet pattern during the COVID-19 lockdown. However, as the COVID-19 pandemic is ongoing, our data need to be confirmed and investigated in future more extensive population studies.","ABSTRACT Heavy metal toxicity has proven to be a major threat and there are several health risks associated with it. The toxic effects of these metals, even though they do not have any biological role, remain present in some or the other form harmful for the human body and its proper functioning. They sometimes act as a pseudo element of the body while at certain times they may even interfere with metabolic processes. Few metals, such as aluminium, can be removed through elimination activities, while some metals get accumulated in the body and food chain, exhibiting a chronic nature. Various public health measures have been undertaken to control, prevent and treat metal toxicity occurring at various levels, such as occupational exposure, accidents and environmental factors. Metal toxicity depends upon the absorbed dose, the route of exposure and duration of exposure, i.e. acute or chronic. This can lead to various disorders and can also result in excessive damage due to oxidative stress induced by free radical formation. This review gives details about some heavy metals and their toxicity mechanisms, along with their health effects","Oxidative stress is a normal phenomenon in the body. Under normal conditions, the physiologically important intracellular levels of reactive oxygen species (ROS) are maintained at low levels by various enzyme systems participating in the in vivo redox homeostasis. Therefore, oxidative stress can also be viewed as an imbalance between the prooxidants and antioxidants in the body. For the last two decades, oxidative stress has been one of the most burning topics among the biological researchers all over the world. Several reasons can be assigned to justify its importance: knowledge about reactive oxygen and nitrogen species production and metabolism; identification of biomarkers for oxidative damage; evidence relating manifestation of chronic and some acute health problems to oxidative stress; identification of various dietary antioxidants present in plant foods as bioactive molecules; and so on. This review discusses the importance of oxidative stress in the body growth and development as well as proteomic and genomic evidences of its relationship with disease development, incidence of malignancies and autoimmune disorders, increased susceptibility to bacterial, viral, and parasitic diseases, and an interplay with prooxidants and antioxidants for maintaining a sound health, which would be helpful in enhancing the knowledge of any biochemist, pathophysiologist, or medical personnel regarding this important issue.",20.71351623142,2,oxid
2,global; increas; cancer; diseas; million; estim; crop; caus; declin; year; oxygen; foodborn; death; data; result,"Each year, the American Cancer Society estimates the numbers of new cancer cases and deaths that will occur in the United States and compiles the most recent data on population‐based cancer occurrence. Incidence data (through 2016) were collected by the Surveillance, Epidemiology, and End Results Program; the National Program of Cancer Registries; and the North American Association of Central Cancer Registries. Mortality data (through 2017) were collected by the National Center for Health Statistics. In 2020, 1,806,590 new cancer cases and 606,520 cancer deaths are projected to occur in the United States. The cancer death rate rose until 1991, then fell continuously through 2017, resulting in an overall decline of 29% that translates into an estimated 2.9 million fewer cancer deaths than would have occurred if peak rates had persisted. This progress is driven by long‐term declines in death rates for the 4 leading cancers (lung, colorectal, breast, prostate); however, over the past decade (2008‐2017), reductions slowed for female breast and colorectal cancers, and halted for prostate cancer. In contrast, declines accelerated for lung cancer, from 3% annually during 2008 through 2013 to 5% during 2013 through 2017 in men and from 2% to almost 4% in women, spurring the largest ever single‐year drop in overall cancer mortality of 2.2% from 2016 to 2017. Yet lung cancer still caused more deaths in 2017 than breast, prostate, colorectal, and brain cancers combined. Recent mortality declines were also dramatic for melanoma of the skin in the wake of US Food and Drug Administration approval of new therapies for metastatic disease, escalating to 7% annually during 2013 through 2017 from 1% during 2006 through 2010 in men and women aged 50 to 64 years and from 2% to 3% in those aged 20 to 49 years; annual declines of 5% to 6% in individuals aged 65 years and older are particularly striking because rates in this age group were increasing prior to 2013. It is also notable that long‐term rapid increases in liver cancer mortality have attenuated in women and stabilized in men. In summary, slowing momentum for some cancers amenable to early detection is juxtaposed with notable gains for other common cancers.","Background Foodborne diseases are important worldwide, resulting in considerable morbidity and mortality. To our knowledge, we present the first global and regional estimates of the disease burden of the most important foodborne bacterial, protozoal, and viral diseases. Methods and Findings We synthesized data on the number of foodborne illnesses, sequelae, deaths, and Disability Adjusted Life Years (DALYs), for all diseases with sufficient data to support global and regional estimates, by age and region. The data sources included varied by pathogen and included systematic reviews, cohort studies, surveillance studies and other burden of disease assessments. We sought relevant data circa 2010, and included sources from 1990–2012. The number of studies per pathogen ranged from as few as 5 studies for bacterial intoxications through to 494 studies for diarrheal pathogens. To estimate mortality for Mycobacterium bovis infections and morbidity and mortality for invasive non-typhoidal Salmonella enterica infections, we excluded cases attributed to HIV infection. We excluded stillbirths in our estimates. We estimate that the 22 diseases included in our study resulted in two billion (95% uncertainty interval [UI] 1.5–2.9 billion) cases, over one million (95% UI 0.89–1.4 million) deaths, and 78.7 million (95% UI 65.0–97.7 million) DALYs in 2010. To estimate the burden due to contaminated food, we then applied proportions of infections that were estimated to be foodborne from a global expert elicitation. Waterborne transmission of disease was not included. We estimate that 29% (95% UI 23–36%) of cases caused by diseases in our study, or 582 million (95% UI 401–922 million), were transmitted by contaminated food, resulting in 25.2 million (95% UI 17.5–37.0 million) DALYs. Norovirus was the leading cause of foodborne illness causing 125 million (95% UI 70–251 million) cases, while Campylobacter spp. caused 96 million (95% UI 52–177 million) foodborne illnesses. Of all foodborne diseases, diarrheal and invasive infections due to non-typhoidal S. enterica infections resulted in the highest burden, causing 4.07 million (95% UI 2.49–6.27 million) DALYs. Regionally, DALYs per 100,000 population were highest in the African region followed by the South East Asian region. Considerable burden of foodborne disease is borne by children less than five years of age. Major limitations of our study include data gaps, particularly in middle- and high-mortality countries, and uncertainty around the proportion of diseases that were foodborne. Conclusions Foodborne diseases result in a large disease burden, particularly in children. Although it is known that diarrheal diseases are a major burden in children, we have demonstrated for the first time the importance of contaminated food as a cause. There is a need to focus food safety interventions on preventing foodborne diseases, particularly in low- and middle-income settings.","Changes in the global production of major crops are important drivers of food prices, food security and land use decisions. Average global yields for these commodities are determined by the performance of crops in millions of fields distributed across a range of management, soil and climate regimes. Despite the complexity of global food supply, here we show that simple measures of growing season temperatures and precipitation—spatial averages based on the locations of each crop—explain ∼30% or more of year-to-year variations in global average yields for the world’s six most widely grown crops. For wheat, maize and barley, there is a clearly negative response of global yields to increased temperatures. Based on these sensitivities and observed climate trends, we estimate that warming since 1981 has resulted in annual combined losses of these three crops representing roughly 40 Mt or $5 billion per year, as of 2002. While these impacts are small relative to the technological yield gains over the same period, the results demonstrate already occurring negative impacts of climate trends on crop yields at the global scale.","Beneath the waves, oxygen disappears As plastic waste pollutes the oceans and fish stocks decline, unseen below the surface another problem grows: deoxygenation. Breitburg et al. review the evidence for the downward trajectory of oxygen levels in increasing areas of the open ocean and coastal waters. Rising nutrient loads coupled with climate change—each resulting from human activities—are changing ocean biogeochemistry and increasing oxygen consumption. This results in destabilization of sediments and fundamental shifts in the availability of key nutrients. In the short term, some compensatory effects may result in improvements in local fisheries, such as in cases where stocks are squeezed between the surface and elevated oxygen minimum zones. In the longer term, these conditions are unsustainable and may result in ecosystem collapses, which ultimately will cause societal and economic harm. Science, this issue p. eaam7240 BACKGROUND Oxygen concentrations in both the open ocean and coastal waters have been declining since at least the middle of the 20th century. This oxygen loss, or deoxygenation, is one of the most important changes occurring in an ocean increasingly modified by human activities that have raised temperatures, CO2 levels, and nutrient inputs and have altered the abundances and distributions of marine species. Oxygen is fundamental to biological and biogeochemical processes in the ocean. Its decline can cause major changes in ocean productivity, biodiversity, and biogeochemical cycles. Analyses of direct measurements at sites around the world indicate that oxygen-minimum zones in the open ocean have expanded by several million square kilometers and that hundreds of coastal sites now have oxygen concentrations low enough to limit the distribution and abundance of animal populations and alter the cycling of important nutrients. ADVANCES In the open ocean, global warming, which is primarily caused by increased greenhouse gas emissions, is considered the primary cause of ongoing deoxygenation. Numerical models project further oxygen declines during the 21st century, even with ambitious emission reductions. Rising global temperatures decrease oxygen solubility in water, increase the rate of oxygen consumption via respiration, and are predicted to reduce the introduction of oxygen from the atmosphere and surface waters into the ocean interior by increasing stratification and weakening ocean overturning circulation. In estuaries and other coastal systems strongly influenced by their watershed, oxygen declines have been caused by increased loadings of nutrients (nitrogen and phosphorus) and organic matter, primarily from agriculture; sewage; and the combustion of fossil fuels. In many regions, further increases in nitrogen discharges to coastal waters are projected as human populations and agricultural production rise. Climate change exacerbates oxygen decline in coastal systems through similar mechanisms as those in the open ocean, as well as by increasing nutrient delivery from watersheds that will experience increased precipitation. Expansion of low-oxygen zones can increase production of N2O, a potent greenhouse gas; reduce eukaryote biodiversity; alter the structure of food webs; and negatively affect food security and livelihoods. Both acidification and increasing temperature are mechanistically linked with the process of deoxygenation and combine with low-oxygen conditions to affect biogeochemical, physiological, and ecological processes. However, an important paradox to consider in predicting large-scale effects of future deoxygenation is that high levels of productivity in nutrient-enriched coastal systems and upwelling areas associated with oxygen-minimum zones also support some of the world’s most prolific fisheries. OUTLOOK Major advances have been made toward understanding patterns, drivers, and consequences of ocean deoxygenation, but there is a need to improve predictions at large spatial and temporal scales important to ecosystem services provided by the ocean. Improved numerical models of oceanographic processes that control oxygen depletion and the large-scale influence of altered biogeochemical cycles are needed to better predict the magnitude and spatial patterns of deoxygenation in the open ocean, as well as feedbacks to climate. Developing and verifying the next generation of these models will require increased in situ observations and improved mechanistic understanding on a variety of scales. Models useful for managing nutrient loads can simulate oxygen loss in coastal waters with some skill, but their ability to project future oxygen loss is often hampered by insufficient data and climate model projections on drivers at appropriate temporal and spatial scales. Predicting deoxygenation-induced changes in ecosystem services and human welfare requires scaling effects that are measured on individual organisms to populations, food webs, and fisheries stocks; considering combined effects of deoxygenation and other ocean stressors; and placing an increased research emphasis on developing nations. Reducing the impacts of other stressors may provide some protection to species negatively affected by low-oxygen conditions. Ultimately, though, limiting deoxygenation and its negative effects will necessitate a substantial global decrease in greenhouse gas emissions, as well as reductions in nutrient discharges to coastal waters. Low and declining oxygen levels in the open ocean and coastal waters affect processes ranging from biogeochemistry to food security. The global map indicates coastal sites where anthropogenic nutrients have exacerbated or caused O2 declines to <2 mg liter−1 (<63 μmol liter−1) (red dots), as well as ocean oxygen-minimum zones at 300 m of depth (blue shaded regions). [Map created from data provided by R. Diaz, updated by members of the GO2NE network, and downloaded from the World Ocean Atlas 2009]. Oxygen is fundamental to life. Not only is it essential for the survival of individual animals, but it regulates global cycles of major nutrients and carbon. The oxygen content of the open ocean and coastal waters has been declining for at least the past half-century, largely because of human activities that have increased global temperatures and nutrients discharged to coastal waters. These changes have accelerated consumption of oxygen by microbial respiration, reduced solubility of oxygen in water, and reduced the rate of oxygen resupply from the atmosphere to the ocean interior, with a wide range of biological and ecological consequences. Further research is needed to understand and predict long-term, global- and regional-scale oxygen changes and their effects on marine and estuarine fisheries and ecosystems.","Honeybees Can't Do It Alone The majority of food crops require pollination to set fruit with the honeybee providing a pollination workhorse, with both feral and managed populations an integral component of crop management (see the Perspective by Tylianakis, published online 28 February). Garibaldi et al. (p. 1608, published online 28 February) now show that wild pollinators are also a vital part of our crop systems. In more than 40 important crops grown worldwide, wild pollinators improved pollination efficiency, increasing fruit set by twice that facilitated by honeybees. Burkle et al. (p. 1611, published online 28 February) took advantage of one of the most thorough and oldest data sets available on plant-pollinator interaction networks and recollected data on plant-pollinator interactions after more than 120 years of climate change and landscape alteration. The historical data set consists of observations collected by Charles Robertson near Carlinville, Illinois (USA), in the late 1800s on the phenology of plants and their pollinating insects, as well as information about which plants and pollinators interacted with one another. Many sites were revisited in the early 1970s and in 2009 and 2010 to collect similar plant-pollinator data. Pollinator function has declined through time, with bees showing lower visitation rates and lower fidelity to individual plant species. Flower visits by wild insects enhanced fruit production in crops worldwide, well beyond the effect of bees. [Also see Perspective by Tylianakis] The diversity and abundance of wild insect pollinators have declined in many agricultural landscapes. Whether such declines reduce crop yields, or are mitigated by managed pollinators such as honey bees, is unclear. We found universally positive associations of fruit set with flower visitation by wild insects in 41 crop systems worldwide. In contrast, fruit set increased significantly with flower visitation by honey bees in only 14% of the systems surveyed. Overall, wild insects pollinated crops more effectively; an increase in wild insect visitation enhanced fruit set by twice as much as an equivalent increase in honey bee visitation. Visitation by wild insects and honey bees promoted fruit set independently, so pollination by managed honey bees supplemented, rather than substituted for, pollination by wild insects. Our results suggest that new practices for integrated management of both honey bees and diverse wild insect assemblages will enhance global crop yields.","To better quantify the impact of foodborne diseases on health in the United States, we compiled and analyzed information from multiple surveillance systems and other sources. We estimate that foodborne diseases cause approximately 76 million illnesses, 325,000 hospitalizations, and 5,000 deaths in the United States each year. Known pathogens account for an estimated 14 million illnesses, 60, 000 hospitalizations, and 1,800 deaths. Three pathogens, Salmonella, Listeria, and Toxoplasma, are responsible for 1,500 deaths each year, more than 75% of those caused by known pathogens, while unknown agents account for the remaining 62 million illnesses, 265,000 hospitalizations, and 3,200 deaths. Overall, foodborne diseases appear to cause more illnesses but fewer deaths than previously estimated.","Illness and death from diseases caused by contaminated food are a constant threat to public health and a significant impediment to socio-economic development worldwide. To measure the global and regional burden of foodborne disease (FBD), the World Health Organization (WHO) established the Foodborne Disease Burden Epidemiology Reference Group (FERG), which here reports their first estimates of the incidence, mortality, and disease burden due to 31 foodborne hazards. We find that the global burden of FBD is comparable to those of the major infectious diseases, HIV/AIDS, malaria and tuberculosis. The most frequent causes of foodborne illness were diarrheal disease agents, particularly norovirus and Campylobacter spp. Diarrheal disease agents, especially non-typhoidal Salmonella enterica, were also responsible for the majority of deaths due to FBD. Other major causes of FBD deaths were Salmonella Typhi, Taenia solium and hepatitis A virus. The global burden of FBD caused by the 31 hazards in 2010 was 33 million Disability Adjusted Life Years (DALYs); children under five years old bore 40% of this burden. The 14 subregions, defined on the basis of child and adult mortality, had considerably different burdens of FBD, with the greatest falling on the subregions in Africa, followed by the subregions in South-East Asia and the Eastern Mediterranean D subregion. Some hazards, such as non-typhoidal S. enterica, were important causes of FBD in all regions of the world, whereas others, such as certain parasitic helminths, were highly localised. Thus, the burden of FBD is borne particularly by children under five years old–although they represent only 9% of the global population–and people living in low-income regions of the world. These estimates are conservative, i.e., underestimates rather than overestimates; further studies are needed to address the data gaps and limitations of the study. Nevertheless, all stakeholders can contribute to improvements in food safety throughout the food chain by incorporating these estimates into policy development at national and international levels.","Despite the importance of microbial communities for ecosystem services and human welfare, the relationship between microbial diversity and multiple ecosystem functions and services (that is, multifunctionality) at the global scale has yet to be evaluated. Here we use two independent, large-scale databases with contrasting geographic coverage (from 78 global drylands and from 179 locations across Scotland, respectively), and report that soil microbial diversity positively relates to multifunctionality in terrestrial ecosystems. The direct positive effects of microbial diversity were maintained even when accounting simultaneously for multiple multifunctionality drivers (climate, soil abiotic factors and spatial predictors). Our findings provide empirical evidence that any loss in microbial diversity will likely reduce multifunctionality, negatively impacting the provision of services such as climate regulation, soil fertility and food and fibre production by terrestrial ecosystems. The role of microbial diversity in ecosystems is less well understood than, for example, that of plant diversity. Analysing two independent data sets at a global and regional scale, Delgado-Baquerizo et al. show positive effects of soil diversity on multiple terrestrial ecosystem functions.","Abstract This report of the European Food Safety Authority and the European Centre for Disease Prevention and Control presents the results of zoonoses monitoring activities carried out in 2017 in 37 European countries (28 Member States (MS) and nine non‐MS). Campylobacteriosis was the commonest reported zoonosis and its EU trend for confirmed human cases increasing since 2008 stabilised during 2013–2017. The decreasing EU trend for confirmed human salmonellosis cases since 2008 ended during 2013–2017, and the proportion of human Salmonella Enteritidis cases increased, mostly due to one MS starting to report serotype data. Sixteen MS met all Salmonella reduction targets for poultry, whereas 12 MS failed meeting at least one. The EU flock prevalence of target Salmonella serovars in breeding hens, laying hens, broilers and fattening turkeys decreased or remained stable compared to 2016, and slightly increased in breeding turkeys. Salmonella results on pig carcases and target Salmonella serovar results for poultry from competent authorities tended to be generally higher compared to those from food business operators. The notification rate of human listeriosis further increased in 2017, despite Listeria seldom exceeding the EU food safety limit in ready‐to‐eat food. The decreasing EU trend for confirmed yersiniosis cases since 2008 stabilised during 2013–2017. The number of confirmed shiga toxin‐producing Escherichia coli (STEC) infections in humans was stable. A total of 5,079 food‐borne (including waterborne) outbreaks were reported. Salmonella was the commonest detected agent with S. Enteritidis causing one out of seven outbreaks, followed by other bacteria, bacterial toxins and viruses. The agent was unknown in 37.6% of all outbreaks. Salmonella in eggs and Salmonella in meat and meat products were the highest risk agent/food pairs. The report further summarises trends and sources for bovine tuberculosis, Brucella, Trichinella, Echinococcus, Toxoplasma, rabies, Coxiella burnetii (Q fever), West Nile virus and tularaemia.","Abiotic stresses are one of the major constraints to crop production and food security worldwide. The situation has aggravated due to the drastic and rapid changes in global climate. Heat and drought are undoubtedly the two most important stresses having huge impact on growth and productivity of the crops. It is very important to understand the physiological, biochemical, and ecological interventions related to these stresses for better management. A wide range of plant responses to these stresses could be generalized into morphological, physiological, and biochemical responses. Interestingly, this review provides a detailed account of plant responses to heat and drought stresses with special focus on highlighting the commonalities and differences. Crop growth and yields are negatively affected by sub-optimal water supply and abnormal temperatures due to physical damages, physiological disruptions, and biochemical changes. Both these stresses have multi-lateral impacts and therefore, complex in mechanistic action. A better understanding of plant responses to these stresses has pragmatic implication for remedies and management. A comprehensive account of conventional as well as modern approaches to deal with heat and drought stresses have also been presented here. A side-by-side critical discussion on salient responses and management strategies for these two important abiotic stresses provides a unique insight into the phenomena. A holistic approach taking into account the different management options to deal with heat and drought stress simultaneously could be a win-win approach in future.",18.2113971599093,0,global
3,patient; diseas; activ; test; can; sever; risk; symptom; fiber; studi; clinic; infect; consumpt; diagnosi; diabet,"Editor's Note: This issue of In the Clinic has been updated. Asthma, which is characterized by airway hyperresponsiveness and inflammation, is one of the most common respiratory illnesses. The global prevalence of asthma is increasing despite the development of new therapeutic approaches. Over the past 20 years, asthma mortality in the United States has declined; however, morbidity, as measured by hospitalizations and emergency department visits, continues to climb. Currently, about 1 in 20 Americans have asthma; in children, recent estimates suggest an incidence as high as 10%. In certain groups of Americans, such as persons of lower socioeconomic status and minority ethnicity, asthma morbidity and mortality are disproportionately high. Such trends are surprising, given the improvement in air quality in the United States and the availability of new pharmacologic therapies. Clinical Slide Set. Asthma Diagnosis What symptoms or elements of clinical history are helpful in diagnosing asthma? Symptoms that should prompt clinicians to consider asthma are wheezing, dyspnea, cough, difficulty taking a deep breath, and chest tightness (1). Characteristically, asthma symptoms are intermittent and may remit spontaneously or with use of short-acting bronchodilators. Symptoms often vary seasonally or are associated with specific triggers, such as cold, exercise, animal dander, pollen, certain foods, aspirin or nonsteroidal anti-inflammatory drugs, or occupational exposures. Clinicians should also consider the diagnosis of asthma in all adults with chronic cough, especially if cough is nocturnal, seasonal, or related to the workplace or a specific activity. What physical examination findings are suggestive of asthma? A careful history to elicit the nature and timing of symptoms is paramount in diagnosing asthma. The physical examination is less helpful unless a patient is having an active exacerbation. The clinician should listen for wheezing during tidal respirations or prolonged expiratory phase of breathing and examine the chest for hyperexpansion. Studies suggest that respiratory signs (wheezing, forced expiratory time, accessory muscle use, respiratory rate, and pulsus paradoxus) may be useful to predict airflow obstruction, but clinicians often disagree about the presence and absence of these signs (1, 2). The physical examination is sometimes most helpful in looking for evidence of alternative diagnoses. Persistent dry inspiratory crackles, focal wet crackles, or an abnormal cardiac examination all suggest diagnoses other than asthma. How can clinicians determine whether asthma is the cause of chronic cough in adults? Coughing may be the only manifestation of asthma in some patients (3). Up to 24% of patients presenting to a specialist with chronic cough after an initial evaluation by a primary care provider may have asthma. Although several protocols are available for the diagnosis of patients with chronic cough, it is not clear which is the best approach. Clinicians often use a trial of empirical asthma therapy, but national guidelines recommend pulmonary function tests for patients with chronic cough of unknown etiology. What are the indications for spirometry in a patient whose clinical presentation is consistent with asthma? Fair-quality evidence supports the performance of spirometry in all adult patients and older children suspected of having asthma. Initial pulmonary function testing should include spirometric measurements of the FEV1, FVC, and the FEV1-FVC ratio. If these measurements reveal airflow obstruction, then they should be repeated after administration of a bronchodilator to evaluate the reversibility of airflow obstruction. Reversibility of airflow obstruction defines asthma. Predicted normal values for spirometric measures are population-based and differ with age and ethnicity. Predictive tables are available (5, 6). Postbronchodilator improvement 12% of the FEV1 or FVC indicates significant reversibility and therefore increases the likelihood of an asthma diagnosis. Complete pulmonary function testing that includes lung volumes and diffusing capacity should be considered when there is evidence of a lack of airflow reversibility, or restrictive patterns with diminutions in the FEV1 and FVC but a normal FEV1-FVC ratio. These findings suggest chronic obstructive pulmonary disease (COPD) or interstitial lung disease (Table 1). Table 1. Laboratory and Other Studies for Asthma A number of studies show a poor correlation among the presence, severity, and timing of wheezing and the degree of airflow obstruction (7, 8). Patients vary in their degree of sensitivity to airflow limitations and can acclimate to the disability and thus become insensitive to airflow obstruction (9). Because of the disparity between patient and physician estimates of the severity of airflow obstruction and objective measures of obstruction, pulmonary function tests are important tools to characterize airflow obstruction and the degree and severity of asthma. Spirometry should adhere to the standards of the American Thoracic Society (10). Of note, spirometry is effort-dependent, and many patients have difficulty with the FVC maneuver. In these patients (younger children, older adults, or patients with severe respiratory disease), alternative approaches, such as the FEV6 may be an acceptable surrogate to the FVC, with a reduction in the FEV1-FEV6 ratio signifying obstruction (11). Does normal spirometry rule out a diagnosis of asthma? Abnormal spirometry (reversible obstruction) can confirm an asthma diagnosis, but normal spirometry does not rule out asthma. Clinicians should consider further studies in patients with normal spirometry who have a clinical history suggestive of asthma (Table 1). Bronchoprovocation with methacholine or histamine can be helpful in establishing a diagnosis in patients who report that they only have symptoms during exercise or at certain times of the year. Alternatively, marked diurnal variability based on measurements recorded in a peak flow diary kept for at least 2 weeks can help to establish asthma as the cause of symptoms. However, peak flow measurements are highly effort-dependent and may offer no opportunity for quality assurance of their accuracy. When should clinicians consider provocative pulmonary testing? A gold standard for diagnosis of asthma remains elusive. However, methacholine hyperresponsiveness in the pulmonary function laboratory has high reproducibility and accepted standardization (12). The test is safe but requires sophisticated instrumentation and is labor-intensive and expensive. In a patient with symptoms suggestive of asthma who has normal baseline spirometry, a low PC20 (the concentration of inhaled methacholine needed to induce a 20% decrease in the FEV1) on methacholine challenge testing supports the diagnosis. Studies of methacholine challenge suggest that it is sensitive and has a high negative predictive value for the diagnosis of asthma (13, 14). Although cold air and exercise have been used in research to define mechanisms of bronchoconstriction, methacholine challenge remains the provocative test of choice in patients with normal pulmonary function tests who have symptoms consistent with asthma. Spirometry before, during, or after exercise may be the only method to document bronchoconstriction in patients with exercise-induced asthma. As an alternative, monitoring peak flow is easy and inexpensive, but the measurement is less precise and limited in reproducibility and sensitivity (15). Because spirometry and peak flow have limitations in sensitivity and specificity, they are probably best used as part of a diagnostic strategy in conjunction with a comprehensive history, physical examination, and other laboratory data (16). How should clinicians classify asthma severity? The National Heart Lung and Blood Institute (NHLBI) Expert Panel Report 2 (2) defines asthma severity according to symptoms and spirometric measurements. As shown in Table 2, asthma severity is classified as intermittent, mild, moderate, and severe persistent. Each category is defined by the frequency of rescue inhaler use as well as nocturnal symptoms in conjunction with the FEV1 or PEFR measurement. It is important to note that decrease in FEV1 correlates with airflow obstruction and not with changes due to restrictive lung disease. Table 2. The Step Classification of Asthma Severity The initial determination of asthma severity should be made when the patient is receiving no medications. Asthma severity is dynamic—for example, patients who were initially diagnosed as having severe persistent asthma may have symptoms consistent with mild persistent asthma while receiving medication. The NHLBI Expert Panel Report 2 (2) suggests annual spirometry to aid in the classification of asthma, but high-quality studies are not available to support this recommendation. What comorbid conditions and alternative diagnoses should clinicians consider in patients with suspected asthma? The differential diagnosis of asthma includes the following conditions: COPD, interstitial lung disease, vocal cord dysfunction, congestive heart failure, medication-induced cough, bronchiectasis, pulmonary infiltration with eosinophilia syndromes, obstructive sleep apnea, mechanical airway obstruction, cystic fibrosis, and pulmonary hypertension. Clinicians should consider one of these alternative diagnoses when asthma is difficult to control or if the patient has atypical signs and symptoms. These conditions can also coexist in a patient who has asthma. An important difference between asthma and COPD is the history of smoking. Although 30% of patients with asthma in the United States smoke, COPD, manifested by chronic bronchitis and emphysema, often occurs in older persons with a substantial history of cigarette smoking. Patients with COPD also do not demonstrate reversibility with bronchodilators on pulmonary function testing. Lung","Editor's Note: This issue of In the Clinic has been updated. One third of the world population has Mycobacterium tuberculosis infection (1). Despite recent progress in the United States, tuberculosis infection remains prevalent in immigrants, immunosuppressed persons, and other high-risk groups (3). Latent tuberculosis infection (LTBI) is the most prevalent form of tuberculosis in the United States (2). LTBI can progress to active tuberculosis disease, especially in individuals with a suppressed cell-mediated immunity. Active tuberculosis disease in immuno-suppressed patients can be difficult to diagnose and can progress to disseminated forms of tuberculosis disease associated with high mortality (4). New methods of diagnosing tuberculosis disease have entered practice in recent years (5), but the diagnosis of LTBI can be challenging in some high-risk populations (6, 7). The introduction of directly observed therapy with first-line antituberculous regimens (8) was an important advance in therapy, but multidrug-resistant tuberculosis (MDR-TB) and the extensively resistant form of MDR-TB remain significant threats to international and local tuberculosis control efforts (9, 10). Screening and Prevention Who should be screened for tuberculosis? Clinicians should screen all individuals at risk for tuberculosis infection, including close contacts of persons who have active pulmonary tuberculosis. Table 1 identifies asymptomatic individuals who should be screened because they are at high risk for exposure to active tuberculosis or at high risk for disease once infected. Table 1. Risk Factors for Tuberculosis Infection or Progression to Disease After Infection What tests are used to screen for tuberculosis? The tuberculin skin test (TST) with purified protein derivative (PPD) and the Mantoux method have been in use for more than 100 years to screen for tuberculosis. The TST result may not become positive for 8 to 10 weeks after exposure to active tuberculosis. The TST can give false-positive results in patient with previous bacille Calmette-Gurin (BCG) vaccination or other mycobacterial infections and false-negative results in anergic or immunosuppressed patients; however, previous BCG vaccination should not change the interpretation of the TST in most adults. The newer interferon- release assays (IGRAs), including the 2 U.S. Food and Drug Administration-approved commercial tests (T-SPOT.TB [Oxford Immunotec, Oxford, United Kingdom], and QuantiFERON-TB Gold and its In-tube version [Cellestis, Valencia, California]) can also be used in circumstances in which the TST is currently used (11). IGRAs assess the T-cell lymphocyte response to specific M. tuberculosis antigens (for example, ESAT-6 and CFP-10) and are more specific, and possibly more sensitive, than TST (12, 13). However, information about IGRA performance is limited in immunocompromised patients and patients receiving immunosuppressive therapy (6, 14). The commercially available IGRAs also have limitations; indeterminate results can occur in immunosuppressed patients, more so with QuantiFERON TB Gold than T-SPOT.TB (6). Discordant results between TST and IGRA testing also occur in about 20% of individuals (13), which could be related, at least in part, to differences in performance characteristics of these tests (5) and to characteristics of the studied populations, such as the prevalence of persons previously vaccinated with BCG and the proportion of persons born outside the United States (15, 16). In addition to their improved specificity compared with TST, IGRAs have several practical advantages. They do not require a second visit for reading and they do not trigger amnestic responses. Longitudinal data supporting the predictive value of IGRA testing is limited, however, in contrast to the many studies of TST for predicting active tuberculosis (17). A recent study from a high-incidence area of tuberculosis in Africa found that initial test results were positive in only 56% of TST testing and 52% of IGRA testing in close household contacts who developed active tuberculosis during 2 years of follow up. Of these close household contacts who developed active tuberculosis, 71% had a positive result with either TST or IGRA during their initial evaluations (18). Another prospective study (19) from a country with a low incidence of tuberculosis suggests that IGRA testing could be more accurate than TST for diagnosing LTBI and for detecting individuals who will progress to active tuberculosis, but more longitudinal data are needed, especially in immunosuppressed individuals. What can patients do to reduce their likelihood of becoming infected with tuberculosis? Tuberculosis is mainly transmitted by the airborne route from a patient with respiratory symptoms, and its ability to infect others decreases significantly after 2 weeks of effective therapy (20, 22). Therefore, prevention of tuberculosis transmission involves promptly identifying and treating patients with active tuberculosis. For hospitalized patients, prevention includes isolating patients with tuberculosis from other patients and strictly applying other hospital infection control practices (23, 24). Patients usually can be removed from airborne infection isolation when they are no longer considered infectious. Patients are no longer infectious when they are on adequate tuberculosis drug therapy, have had a significant clinical response to therapy, and have had negative results on 3 consecutive sputum smears for acid-fast bacilli (AFB). Some patients can be isolated from outsiders at home after appropriate evaluation and the initiation of outpatient treatment. Isolation of patients at home assumes that household contacts already have been exposed and that further exposure will not affect their outcomes. Two studies, one in India and one in Arkansas, showed similar rates of disease or infection in exposed household contacts whether the patient was admitted to the hospital or allowed to remain at home for initial treatment (25, 26). However, if household contacts of the patients with infectious tuberculosis are at high risk (for example, infants or immuno-compromised persons), housing the patient elsewhere until he or she meets noninfectious criteria should be strongly considered. Hospitalization may be required until housing can be obtained (27). Educating health care workers to evaluate exposed persons for active tuberculosis by obtaining sputum for AFB testing when they have respiratory symptoms has been shown to improve the case detection rates in primary care settings (28). What should clinicians tell patients with active tuberculosis to protect household members and other contacts from infection? Clinicians should teach patients to cough into disposable tissues and to cover their nose and mouth when coughing or sneezing to contain droplet nuclei before they are expelled into the air. Patients who are placed in airborne infection isolation rooms should be educated about the transmission of tuberculosis, the reasons for isolation, and the importance of staying in their rooms. Every effort should be made to help the patient follow the isolation policy (29). Hospital employees and physicians who come in contact with an infectious or suspected infectious patient should wear previously fitted particulate respirators certified by the National Institute for Occupational Safety and Health for protection against tuberculosis, which does not include surgical masks. What are the physician's public health responsibilities after making a diagnosis of active tuberculosis? All 50 U.S. states require physicians to notify public health authorities about all patients suspected of having active tuberculosis (22, 23), which can enable identification of other cases and potentially prevent further transmission of tuberculosis in the community. Genetic fingerprinting of tuberculosis isolates during an outbreak can help public health authorities detect tuberculosis infection in the community (30). Clinical Bottom Line: Screening and Prevention Clinicians should screen persons who have close contact with a person who has active pulmonary tuberculosis, and screen other persons who are at high risk for infection or for progression to disease once infected. Clinicians should screen with TST or IGRAs and should prevent infection by identifying and treating persons with active pulmonary tuberculosis. Patient airborn infection isolation is an important part of early treatment and prevention of transmission. Persons who provide care to patients with active pulmonary tuberculosis should wear particulate respirators. Clinicians should notify public health authorities about patients with suspected active tuberculosis. Diagnosis What signs and symptoms suggest active tuberculosis? Although tuberculosis can cause disease in many parts of the body, this article focuses on pulmonary tuberculosis because it is the most common form of the disease. Clinicians should consider a diagnosis of pulmonary tuberculosis and evaluate patients for tuberculosis if the patient has constitutional or pulmonary signs and symptoms, such as cough longer than 2 to 3 weeks (may not be productive until later in course of disease), hemoptysis (more likely with cavitation and rarely a presenting symptom), chest pain, fever, chills, night sweats, weight loss, easy fatigability, or anorexia. Some patients have classic signs and symptoms, but it is rare for someone to have most of the classic signs and symptoms except in advanced disease, and many patients will have few of them. Some patients with active pulmonary tuberculosis infection can be fairly asymptomatic. Table 2 shows some of the main findings from the history and the physical examination that are associated with active tuberculosis disease. Table 2. Findings from the History and Physical Examination in Patients with Active Tuberculosis One study reviewed 101 patients admitted to respiratory isolation to rul","OBJECTIVE The objective was to provide guidelines to clinicians for the evaluation, treatment, and prevention of vitamin D deficiency with an emphasis on the care of patients who are at risk for deficiency. PARTICIPANTS The Task Force was composed of a Chair, six additional experts, and a methodologist. The Task Force received no corporate funding or remuneration. CONSENSUS PROCESS Consensus was guided by systematic reviews of evidence and discussions during several conference calls and e-mail communications. The draft prepared by the Task Force was reviewed successively by The Endocrine Society's Clinical Guidelines Subcommittee, Clinical Affairs Core Committee, and cosponsoring associations, and it was posted on The Endocrine Society web site for member review. At each stage of review, the Task Force received written comments and incorporated needed changes. CONCLUSIONS Considering that vitamin D deficiency is very common in all age groups and that few foods contain vitamin D, the Task Force recommended supplementation at suggested daily intake and tolerable upper limit levels, depending on age and clinical circumstances. The Task Force also suggested the measurement of serum 25-hydroxyvitamin D level by a reliable assay as the initial diagnostic test in patients at risk for deficiency. Treatment with either vitamin D(2) or vitamin D(3) was recommended for deficient patients. At the present time, there is not sufficient evidence to recommend screening individuals who are not at risk for deficiency or to prescribe vitamin D to attain the noncalcemic benefit for cardiovascular protection.","Dietary fiber intake provides many health benefits. However, average fiber intakes for US children and adults are less than half of the recommended levels. Individuals with high intakes of dietary fiber appear to be at significantly lower risk for developing coronary heart disease, stroke, hypertension, diabetes, obesity, and certain gastrointestinal diseases. Increasing fiber intake lowers blood pressure and serum cholesterol levels. Increased intake of soluble fiber improves glycemia and insulin sensitivity in non-diabetic and diabetic individuals. Fiber supplementation in obese individuals significantly enhances weight loss. Increased fiber intake benefits a number of gastrointestinal disorders including the following: gastroesophageal reflux disease, duodenal ulcer, diverticulitis, constipation, and hemorrhoids. Prebiotic fibers appear to enhance immune function. Dietary fiber intake provides similar benefits for children as for adults. The recommended dietary fiber intakes for children and adults are 14 g/1000 kcal. More effective communication and consumer education is required to enhance fiber consumption from foods or supplements.","Celiac disease is a chronic intestinal disease caused by intolerance to gluten. It is characterized by immune-mediated enteropathy, associated with maldigestion and malabsorption of most nutrients and vitamins. In predisposed individuals, the ingestion of gluten-containing food such as wheat and rye induces a flat jejunal mucosa with infiltration of lymphocytes. The main symptoms are: stomach pain, gas, and bloating, diarrhea, weight loss, anemia, edema, bone or joint pain. Prevalence for clinically overt celiac disease varies from 1:270 in Finland to 1:5000 in North America. Since celiac disease can be asymptomatic, most subjects are not diagnosed or they can present with atypical symptoms. Furthermore, severe inflammation of the small bowel can be present without any gastrointestinal symptoms. The diagnosis should be made early since celiac disease causes growth retardation in untreated children and atypical symptoms like infertility or neurological symptoms. Diagnosis requires endoscopy with jejunal biopsy. In addition, tissue-transglutaminase antibodies are important to confirm the diagnosis since there are other diseases which can mimic celiac disease. The exact cause of celiac disease is unknown but is thought to be primarily immune mediated (tissue-transglutaminase autoantigen); often the disease is inherited. Management consists in life long withdrawal of dietary gluten, which leads to significant clinical and histological improvement. However, complete normalization of histology can take years.","BackgroundVegetables and fruit provide a significant part of human nutrition, as they are important sources of nutrients, dietary fibre, and phytochemicals. However, it is uncertain whether the risk of certain chronic diseases can be reduced by increased consumption of vegetables or fruit by the general public, and what strength of evidence has to be allocated to such an association.MethodsTherefore, a comprehensive analysis of the studies available in the literature and the respective study results has been performed and evaluated regarding obesity, type 2 diabetes mellitus, hypertension, coronary heart disease (CHD), stroke, cancer, chronic inflammatory bowel disease (IBD), rheumatoid arthritis (RA), chronic obstructive pulmonary disease (COPD), asthma, osteoporosis, eye diseases, and dementia. For judgement, the strength of evidence for a risk association, the level of evidence, and the number of studies were considered, the quality of the studies and their estimated relevance based on study design and size.ResultsFor hypertension, CHD, and stroke, there is convincing evidence that increasing the consumption of vegetables and fruit reduces the risk of disease. There is probable evidence that the risk of cancer in general is inversely associated with the consumption of vegetables and fruit. In addition, there is possible evidence that an increased consumption of vegetables and fruit may prevent body weight gain. As overweight is the most important risk factor for type 2 diabetes mellitus, an increased consumption of vegetables and fruit therefore might indirectly reduces the incidence of type 2 diabetes mellitus. Independent of overweight, there is probable evidence that there is no influence of increased consumption on the risk of type 2 diabetes mellitus. There is possible evidence that increasing the consumption of vegetables and fruit lowers the risk of certain eye diseases, dementia and the risk of osteoporosis. Likewise, current data on asthma, COPD, and RA indicate that an increase in vegetable and fruit consumption may contribute to the prevention of these diseases. For IBD, glaucoma, and diabetic retinopathy, there was insufficient evidence regarding an association with the consumption of vegetables and fruit.ConclusionsThis critical review on the associations between the intake of vegetables and fruit and the risk of several chronic diseases shows that a high daily intake of these foods promotes health. Therefore, from a scientific point of view, national campaigns to increase vegetable and fruit consumption are justified. The promotion of vegetable and fruit consumption by nutrition and health policies is a preferable strategy to decrease the burden of several chronic diseases in Western societies.","Type 2 diabetes is a global public health crisis that threatens the economies of all nations, particularly developing countries. Fueled by rapid urbanization, nutrition transition, and increasingly sedentary lifestyles, the epidemic has grown in parallel with the worldwide rise in obesity. Asia's large population and rapid economic development have made it an epicenter of the epidemic. Asian populations tend to develop diabetes at younger ages and lower BMI levels than Caucasians. Several factors contribute to accelerated diabetes epidemic in Asians, including the “normal-weight metabolically obese” phenotype; high prevalence of smoking and heavy alcohol use; high intake of refined carbohydrates (e.g., white rice); and dramatically decreased physical activity levels. Poor nutrition in utero and in early life combined with overnutrition in later life may also play a role in Asia's diabetes epidemic. Recent advances in genome-wide association studies have contributed substantially to our understanding of diabetes pathophysiology, but currently identified genetic loci are insufficient to explain ethnic differences in diabetes risk. Nonetheless, interactions between Westernized diet and lifestyle and genetic background may accelerate the growth of diabetes in the context of rapid nutrition transition. Epidemiologic studies and randomized clinical trials show that type 2 diabetes is largely preventable through diet and lifestyle modifications. Translating these findings into practice, however, requires fundamental changes in public policies, the food and built environments, and health systems. To curb the escalating diabetes epidemic, primary prevention through promotion of a healthy diet and lifestyle should be a global public policy priority.",AbstractAn expert report aiming to design strategies in promoting healthy diets and physical activity behaviours was published a year ago by the United Nations Food and Agriculture Organization (FA...,"correctly identified E. faecium and E. faecalis to the species level, most (4 of 5) did not correctly identify E. gallinarum (three misidentified it as E. casseliflavus and one as E. faecalis). The results of this study are consistent with those of previous studies in the United States (4,5), South America (6), Spain (7), and Mexico (8). Although in countries like Chile, disk diffusion is practical and reliable for most susceptibility testing, detecting low-level vancomycin resistance in enterocci is difficult without supplementary testing. In Chile, as in other countries, strategies should be implemented to improve detection of these strains, including improvement of phenotypical and genotypical methods for VRE detection and species identification. Documentation of proficiency in detecting VRE is important for improving laboratory performance, detecting clinical isolates, and accurate and reliable reporting to local, national, and international surveillance systems.","Background Meta-analyses of antidepressant medications have reported only modest benefits over placebo treatment, and when unpublished trial data are included, the benefit falls below accepted criteria for clinical significance. Yet, the efficacy of the antidepressants may also depend on the severity of initial depression scores. The purpose of this analysis is to establish the relation of baseline severity and antidepressant efficacy using a relevant dataset of published and unpublished clinical trials. Methods and Findings We obtained data on all clinical trials submitted to the US Food and Drug Administration (FDA) for the licensing of the four new-generation antidepressants for which full datasets were available. We then used meta-analytic techniques to assess linear and quadratic effects of initial severity on improvement scores for drug and placebo groups and on drug–placebo difference scores. Drug–placebo differences increased as a function of initial severity, rising from virtually no difference at moderate levels of initial depression to a relatively small difference for patients with very severe depression, reaching conventional criteria for clinical significance only for patients at the upper end of the very severely depressed category. Meta-regression analyses indicated that the relation of baseline severity and improvement was curvilinear in drug groups and showed a strong, negative linear component in placebo groups. Conclusions Drug–placebo differences in antidepressant efficacy increase as a function of baseline severity, but are relatively small even for severely depressed patients. The relationship between initial severity and antidepressant efficacy is attributable to decreased responsiveness to placebo among very severely depressed patients, rather than to increased responsiveness to medication.",13.8017795976417,4,patient
4,drug; data; use; sequenc; differ; genom; new; analysi; inform; relat; also; provid; gene; interact; includ,"DrugBank is a richly annotated resource that combines detailed drug data with comprehensive drug target and drug action information. Since its first release in 2006, DrugBank has been widely used to facilitate in silico drug target discovery, drug design, drug docking or screening, drug metabolism prediction, drug interaction prediction and general pharmaceutical education. The latest version of DrugBank (release 2.0) has been expanded significantly over the previous release. With ∼4900 drug entries, it now contains 60% more FDA-approved small molecule and biotech drugs including 10% more ‘experimental’ drugs. Significantly, more protein target data has also been added to the database, with the latest version of DrugBank containing three times as many non-redundant protein or drug target sequences as before (1565 versus 524). Each DrugCard entry now contains more than 100 data fields with half of the information being devoted to drug/chemical data and the other half devoted to pharmacological, pharmacogenomic and molecular biological data. A number of new data fields, including food–drug interactions, drug–drug interactions and experimental ADME data have been added in response to numerous user requests. DrugBank has also significantly improved the power and simplicity of its structure query and text query searches. DrugBank is available at http://www.drugbank.ca","Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.","DrugBank (http://www.drugbank.ca) is a comprehensive online database containing extensive biochemical and pharmacological information about drugs, their mechanisms and their targets. Since it was first described in 2006, DrugBank has rapidly evolved, both in response to user requests and in response to changing trends in drug research and development. Previous versions of DrugBank have been widely used to facilitate drug and in silico drug target discovery. The latest update, DrugBank 4.0, has been further expanded to contain data on drug metabolism, absorption, distribution, metabolism, excretion and toxicity (ADMET) and other kinds of quantitative structure activity relationships (QSAR) information. These enhancements are intended to facilitate research in xenobiotic metabolism (both prediction and characterization), pharmacokinetics, pharmacodynamics and drug design/discovery. For this release, >1200 drug metabolites (including their structures, names, activity, abundance and other detailed data) have been added along with >1300 drug metabolism reactions (including metabolizing enzymes and reaction types) and dozens of drug metabolism pathways. Another 30 predicted or measured ADMET parameters have been added to each DrugCard, bringing the average number of quantitative ADMET values for Food and Drug Administration-approved drugs close to 40. Referential nuclear magnetic resonance and MS spectra have been added for almost 400 drugs as well as spectral and mass matching tools to facilitate compound identification. This expanded collection of drug information is complemented by a number of new or improved search tools, including one that provides a simple analyses of drug–target, –enzyme and –transporter associations to provide insight on drug–drug interactions.","When we are constantly and simultaneously bombarded with various types of sensory inputs, how do we make decisions or motor outputs to deal with the world in a meaningful manner? This is a central issue in multisensory integration. One example of such type of processing is eating, as placing food in one's mouth provides taste, olfactory and somatosensory (texture, temperature) information (Verhagen and Engelen, 2006, Simon et al., 2006). Indeed, virtually all combinations of the senses are multimodal (Verhagen and Engelen, 2006). Whatever the combination of modalities, information from widely separated areas is rapidly integrated to give a motor response. In the case of taste the response to swallow or reject is yielded. One possibility that our brains have evolved to deal with multisensory stimuli could be due to the combined response reveals more about the nature of the external event. Our grand challenge is to know how information from different modalities that originally seem to be processed as unimodal events is combined during development to produce a coherent percept. This is known as the “binding problem” (Singer, 2001) and is important to all multisensory processing. Many issues regarding multisensory processing are found in a very recent and excellent review by Stein and Stanford (Stein and Stranford, 2008), and given the space limitations, I will not repeat them but rather give my opinion as to what is missing and what experiments would lead to a better understanding. It has been proposed that the entire neocortex is multisensory (Ghazanfar and Schroeder, 2006) and, although there has been some controversy about this proposal, it would be useful to have an atlas of the multisensory specificity of all subcortical and cortical areas. In electrophysiological studies of multisensory integration most recordings have been from single units, and confined to a single area. No doubt these recordings have lead to many advances and the formulation of rules that govern multisensory processing (e.g. superadditivity (Stein and Stranford, 2008), but it would be more informative if ensembles of neurons could be recorded in different areas while animals are being tested. This could be done either with implanted arrays of electrodes or with optical imaging (Nicolelis, 2007; Gradinaru, 2007). In this manner we will learn how different neurons in the same and different areas interact (synchronously or asynchronously) at different time scales. Such details can be use to determine which area signified the event (say congruent or non-congruent), even on a single trial (Nicolelis, 2007). We will also understand how the temporal structure of the neuronal dynamics provides information as to the behavioal task. The new technique involving the reversible use of light-induced activation or inactivation (Gradinaru, 2007) of neuronal ensembles in a given area will enable researchers to directly determine how the input of one modality affects the other modalitities and to determine whether multisensory information is feedforward and/or arises from top-down influences. This method may also permit researchers to begin to understand how animals determine saliency of cues to the extent as to which sensory modality yields most information. With these advances in animal research together with fMRI, EEG and evoked potential experiments in humans much progress will be made towards achieving our grand challenge – namely, to understand how the many facets of the sensory world around us becomes one coherent percept in our brain.","DrugBank (http://www.drugbank.ca) is a richly annotated database of drug and drug target information. It contains extensive data on the nomenclature, ontology, chemistry, structure, function, action, pharmacology, pharmacokinetics, metabolism and pharmaceutical properties of both small molecule and large molecule (biotech) drugs. It also contains comprehensive information on the target diseases, proteins, genes and organisms on which these drugs act. First released in 2006, DrugBank has become widely used by pharmacists, medicinal chemists, pharmaceutical researchers, clinicians, educators and the general public. Since its last update in 2008, DrugBank has been greatly expanded through the addition of new drugs, new targets and the inclusion of more than 40 new data fields per drug entry (a 40% increase in data ‘depth’). These data field additions include illustrated drug-action pathways, drug transporter data, drug metabolite data, pharmacogenomic data, adverse drug response data, ADMET data, pharmacokinetic data, computed property data and chemical classification data. DrugBank 3.0 also offers expanded database links, improved search tools for drug–drug and food–drug interaction, new resources for querying and viewing drug pathways and hundreds of new drug entries with detailed patent, pricing and manufacturer data. These additions have been complemented by enhancements to the quality and quantity of existing data, particularly with regard to drug target, drug description and drug action data. DrugBank 3.0 represents the result of 2 years of manual annotation work aimed at making the database much more useful for a wide range of ‘omics’ (i.e. pharmacogenomic, pharmacoproteomic, pharmacometabolomic and even pharmacoeconomic) applications.","Deblur provides a rapid and sensitive means to assess ecological patterns driven by differentiation of closely related taxa. This algorithm provides a solution to the problem of identifying real ecological differences between taxa whose amplicons differ by a single base pair, is applicable in an automated fashion to large-scale sequencing data sets, and can integrate sequencing runs collected over time. ABSTRACT High-throughput sequencing of 16S ribosomal RNA gene amplicons has facilitated understanding of complex microbial communities, but the inherent noise in PCR and DNA sequencing limits differentiation of closely related bacteria. Although many scientific questions can be addressed with broad taxonomic profiles, clinical, food safety, and some ecological applications require higher specificity. Here we introduce a novel sub-operational-taxonomic-unit (sOTU) approach, Deblur, that uses error profiles to obtain putative error-free sequences from Illumina MiSeq and HiSeq sequencing platforms. Deblur substantially reduces computational demands relative to similar sOTU methods and does so with similar or better sensitivity and specificity. Using simulations, mock mixtures, and real data sets, we detected closely related bacterial sequences with single nucleotide differences while removing false positives and maintaining stability in detection, suggesting that Deblur is limited only by read length and diversity within the amplicon sequences. Because Deblur operates on a per-sample level, it scales to modern data sets and meta-analyses. To highlight Deblur’s ability to integrate data sets, we include an interactive exploration of its application to multiple distinct sequencing rounds of the American Gut Project. Deblur is open source under the Berkeley Software Distribution (BSD) license, easily installable, and downloadable from https://github.com/biocore/deblur . IMPORTANCE Deblur provides a rapid and sensitive means to assess ecological patterns driven by differentiation of closely related taxa. This algorithm provides a solution to the problem of identifying real ecological differences between taxa whose amplicons differ by a single base pair, is applicable in an automated fashion to large-scale sequencing data sets, and can integrate sequencing runs collected over time.","Abstract The evolution in next-generation sequencing (NGS) technology has led to the development of many different assembly algorithms, but few of them focus on assembling the organelle genomes. These genomes are used in phylogenetic studies, food identification and are the most deposited eukaryotic genomes in GenBank. Producing organelle genome assembly from whole genome sequencing (WGS) data would be the most accurate and least laborious approach, but a tool specifically designed for this task is lacking. We developed a seed-and-extend algorithm that assembles organelle genomes from whole genome sequencing (WGS) data, starting from a related or distant single seed sequence. The algorithm has been tested on several new (Gonioctena intermedia and Avicennia marina) and public (Arabidopsis thaliana and Oryza sativa) whole genome Illumina data sets where it outperforms known assemblers in assembly accuracy and coverage. In our benchmark, NOVOPlasty assembled all tested circular genomes in less than 30 min with a maximum memory requirement of 16 GB and an accuracy over 99.99%. In conclusion, NOVOPlasty is the sole de novo assembler that provides a fast and straightforward extraction of the extranuclear genomes from WGS data in one circular high quality contig. The software is open source and can be downloaded at https://github.com/ndierckx/NOVOPlasty.","It can be said that we inhabit a post-human world—an existence characterised by smartphones and social media, genetically modified food and IVF babies, life-extending technologies and prosthetic enhancements. In The Posthuman (2013), Rosi Braidotti offers a roadmap for navigating the global effects of this post-human predicament—one in which clear distinctions between the human and the non-human no longer hold, the nature–culture divide is destabilised, and man’s privileged status is under attack. The situation we find ourselves in, Braidotti argues, is neither dystopian technological nightmare nor futuristic fantasy but one that requires complex and nuanced critical responses to issues of subjectivity, ethics and politics. In the four chapters comprising The Posthuman, Braidotti outlines her vision of the post-human future based on an affirmative politics, which ‘combines critique with creativity in the pursuit of alternative visions and projects’ (54). As a feminist antihumanist, Braidotti expresses little nostalgia for the concept of ‘Man’ and its associated individualism, Eurocentrism and anthropocentrism. Chapter 1 ‘Post-Humanism: Life Beyond the Self’ charts the Humanist/anti-humanist debates to draw attention to the crisis of the human and the opportunity it affords to imagine alternative subjectivities grounded in relationality and the interconnection between the self and others (49). Methodologically, Braidotti adopts a feminist politics of location in her critique of various Humanist traditions. There is a profound reflexivity to her writing as she guides the reader through the intellectual trajectory that has resulted in her nomadic, affirmative politics. It is a legacy that incorporates social movements of the 1960s/1970s, as well as the continental feminism of Irigaray and Kristeva. Spinoza and Deleuze and Guattari also feature as philosophical touchstones from which she advances her vision for the posthuman as a ‘relational subject constituted in and by multiplicity, that is to say a subject that works across differences and is also internally differentiated, but still grounded and accountable’ (49). By framing her argument within the narrative of her own intellectual story, the book conveys an immediacy and intimacy not often found in academic prose. Stylistically, it is as though we are inside her head—a post-human experience, indeed. Her writing is as expansive and impressive as you would expect—a swarm of ideas assuredly curated into a compelling argument for generating new forms of subjectivity and ethical relations to confront the challenges of a post-human existence. Consistent with existing feminist appraisals of the post-human (Halberstam and Livingston 1995; Hayles 1999; Toffoletti 2007), Braidotti acknowledges the complexity of the post-human predicament, seeking alternative frameworks to think about post-human subjectivity in non-dualistic ways. What Braidotti brings to these debates is an emphasis on materialism by way of Spinozist monism. In championing the relational, embodied and embedded qualities of post-human existence, Braidotti reprises the concept of zoe—a generative and vitalist force that allows for connections and affinities to be made across","Rice is a staple food for the majority of our world’s growing population. Whereas Asian rice (Oryza sativa L.) has been extensively studied, the exact origins of African rice (Oryza glaberrima Steud.) are still contested. Previous studies have supported either a centric or a non-centric origin of African rice domestication. Here we review the evidence for both scenarios through a critical reassessment of 206 whole genome sequences of domesticated and wild African rice. While genetic diversity analyses support a severe bottleneck caused by domestication, signatures of recent and strong positive selection do not unequivocally point to candidate domestication genes, suggesting that domestication proceeded differently than in Asian rice – either by selection on different alleles, or different modes of selection. Population structure analysis revealed five genetic clusters localising to different geographic regions. Isolation by distance was identified in the coastal populations, which could account for parallel adaptation in geographically separated demes. Although genome-wide phylogenetic relationships support an origin in the eastern cultivation range followed by diversification along the Atlantic coast, further analysis of domestication genes shows distinct haplotypes in the southwest - suggesting that at least one of several key domestication traits might have originated there. These findings shed new light on an old controversy concerning plant domestication in Africa by highlighting the divergent roots of African rice cultivation, including a separate centre of domestication activity in the Guinea Highlands. We thus suggest that the commonly accepted centric origin of African rice must be reconsidered in favour of a non-centric or polycentric view.","The evolution of ""niche breadth,"" or ""niche width,"" was a more popular topic in the evolutionary ecological literature of the 1960s and 1970s than it has been recently (109, 118, 120, 134, 155, 156). This review summarizes current hypotheses on the evolution of specialization and generalization and suggests areas in which future research might be rewarding. The topic is so broad that every area of biology bears on it. We cannot hope to offer an exhaustive review of evidence and in particular have slighted much of the ecological literature to emphasize genetic and evolutionary perspectives. We limit our discussion almost entirely to animals. We adopt Hutchinson's (86) representation of a population's ecological niche as an n-dimensional hypervolume, the axes of which are environmental variables or resources. Along each of these, the population displays a wide or narrow tolerance or pattern of utilization, relative to other populations or species. Specialization and generalization must be defined with reference to particular axes (e.g. temperature, range of food particle sizes). Brown (9) suggests that niche breadth along different axes is positively correlated and that this explains positive correlations across species between local abundance and breadth of geographic range. Multidimensional specialization might be expected if species arise in localized regions that differ in several ecological respects from those occupied by parent species. Cody (20), however, suggested that the breadth of habitat is negatively correlated with diet breadth among certain bird species. In practice, quantitative measurement of niche breadth can be difficult (22,",22.6338768608441,3,drug
5,product; effect; health; use; pesticid; human; environment; intestin; agricultur; plant; increas; environ; prebiot; includ; potenti,"Pesticides are widely used in agricultural production to prevent or control pests, diseases, weeds, and other plant pathogens in an effort to reduce or eliminate yield losses and maintain high product quality. Although pesticides are developed through very strict regulation processes to function with reasonable certainty and minimal impact on human health and the environment, serious concerns have been raised about health risks resulting from occupational exposure and from residues in food and drinking water. Occupational exposure to pesticides often occurs in the case of agricultural workers in open fields and greenhouses, workers in the pesticide industry, and exterminators of house pests. Exposure of the general population to pesticides occurs primarily through eating food and drinking water contaminated with pesticide residues, whereas substantial exposure can also occur in or around the home. Regarding the adverse effects on the environment (water, soil and air contamination from leaching, runoff, and spray drift, as well as the detrimental effects on wildlife, fish, plants, and other non-target organisms), many of these effects depend on the toxicity of the pesticide, the measures taken during its application, the dosage applied, the adsorption on soil colloids, the weather conditions prevailing after application, and how long the pesticide persists in the environment. Therefore, the risk assessment of the impact of pesticides either on human health or on the environment is not an easy and particularly accurate process because of differences in the periods and levels of exposure, the types of pesticides used (regarding toxicity and persistence), and the environmental characteristics of the areas where pesticides are usually applied. Also, the number of the criteria used and the method of their implementation to assess the adverse effects of pesticides on human health could affect risk assessment and would possibly affect the characterization of the already approved pesticides and the approval of the new compounds in the near future. Thus, new tools or techniques with greater reliability than those already existing are needed to predict the potential hazards of pesticides and thus contribute to reduction of the adverse effects on human health and the environment. On the other hand, the implementation of alternative cropping systems that are less dependent on pesticides, the development of new pesticides with novel modes of action and improved safety profiles, and the improvement of the already used pesticide formulations towards safer formulations (e.g., microcapsule suspensions) could reduce the adverse effects of farming and particularly the toxic effects of pesticides. In addition, the use of appropriate and well-maintained spraying equipment along with taking all precautions that are required in all stages of pesticide handling could minimize human exposure to pesticides and their potential adverse effects on the environment.","The intestinal microbiota, epithelium, and immune system provide resistance to enteric pathogens. Recent data suggest that resistance is not solely due to the sum of the components, but that cross-talk between these components is also involved in modulating this resistance. Inhibition of pathogens by the intestinal microbiota has been called bacterial antagonism, bacterial interference, barrier effect, colonization resistance, and competitive exclusion. Mechanisms by which the indigenous intestinal bacteria inhibit pathogens include competition for colonization sites, competition for nutrients, production of toxic compounds, or stimulation of the immune system. These mechanisms are not mutually exclusive, and inhibition may comprise one, several, or all of these mechanisms. Consumption of fermented foods has been associated with improved health, and lactic acid bacteria (lactobacilli and bifidobacteria) have been implicated as the causative agents for this improved health. Research over the last century has shown that lactic acid bacteria and certain other microorganisms can increase resistance to disease and that lactic acid bacteria can be enriched in the intestinal tract by feeding specific carbohydrates. Increased bacterial resistance to antibiotics in humans has caused an increase in public and governmental interest in eliminating sub-therapeutic use of antibiotics in livestock. An alternative approach to sub-therapeutic antibiotics in livestock is the use of probiotic microorganisms, prebiotic substrates that enrich certain bacterial populations, or synbiotic combinations of prebiotics and probiotics. Research is focused on identifying beneficial bacterial strains and substrates along with the conditions under which they are effective.","The human gastrointestinal tract is colonised by a complex ecosystem of microorganisms. Intestinal bacteria are not only commensal, but they also undergo a synbiotic co-evolution along with their host. Beneficial intestinal bacteria have numerous and important functions, e.g., they produce various nutrients for their host, prevent infections caused by intestinal pathogens, and modulate a normal immunological response. Therefore, modification of the intestinal microbiota in order to achieve, restore, and maintain favourable balance in the ecosystem, and the activity of microorganisms present in the gastrointestinal tract is necessary for the improved health condition of the host. The introduction of probiotics, prebiotics, or synbiotics into human diet is favourable for the intestinal microbiota. They may be consumed in the form of raw vegetables and fruit, fermented pickles, or dairy products. Another source may be pharmaceutical formulas and functional food. This paper provides a review of available information and summarises the current knowledge on the effects of probiotics, prebiotics, and synbiotics on human health. The mechanism of beneficial action of those substances is discussed, and verified study results proving their efficacy in human nutrition are presented.","Data are accumulating that emphasize the important role of the intestinal barrier and intestinal permeability for health and disease. However, these terms are poorly defined, their assessment is a matter of debate, and their clinical significance is not clearly established. In the present review, current knowledge on mucosal barrier and its role in disease prevention and therapy is summarized. First, the relevant terms ‘intestinal barrier’ and ‘intestinal permeability’ are defined. Secondly, the key element of the intestinal barrier affecting permeability are described. This barrier represents a huge mucosal surface, where billions of bacteria face the largest immune system of our body. On the one hand, an intact intestinal barrier protects the human organism against invasion of microorganisms and toxins, on the other hand, this barrier must be open to absorb essential fluids and nutrients. Such opposing goals are achieved by a complex anatomical and functional structure the intestinal barrier consists of, the functional status of which is described by ‘intestinal permeability’. Third, the regulation of intestinal permeability by diet and bacteria is depicted. In particular, potential barrier disruptors such as hypoperfusion of the gut, infections and toxins, but also selected over-dosed nutrients, drugs, and other lifestyle factors have to be considered. In the fourth part, the means to assess intestinal permeability are presented and critically discussed. The means vary enormously and probably assess different functional components of the barrier. The barrier assessments are further hindered by the natural variability of this functional entity depending on species and genes as well as on diet and other environmental factors. In the final part, we discuss selected diseases associated with increased intestinal permeability such as critically illness, inflammatory bowel diseases, celiac disease, food allergy, irritable bowel syndrome, and – more recently recognized – obesity and metabolic diseases. All these diseases are characterized by inflammation that might be triggered by the translocation of luminal components into the host. In summary, intestinal permeability, which is a feature of intestinal barrier function, is increasingly recognized as being of relevance for health and disease, and therefore, this topic warrants more attention.","The worldwide increases in both environmental damage and human population pressure have the unfortunate consequence that global food production may soon become insufficient to feed all of the world's people. It is therefore essential that agricultural productivity be significantly increased within the next few decades. To this end, agricultural practice is moving toward a more sustainable and environmentally friendly approach. This includes both the increasing use of transgenic plants and plant growth-promoting bacteria as a part of mainstream agricultural practice. Here, a number of the mechanisms utilized by plant growth-promoting bacteria are discussed and considered. It is envisioned that in the not too distant future, plant growth-promoting bacteria (PGPB) will begin to replace the use of chemicals in agriculture, horticulture, silviculture, and environmental cleanup strategies. While there may not be one simple strategy that can effectively promote the growth of all plants under all conditions, some of the strategies that are discussed already show great promise.","Due to the widespread use and durability of synthetic polymers, plastic debris occurs in the environment worldwide. In the present work, information on sources and fate of microplastic particles in the aquatic and terrestrial environment, and on their uptake and effects, mainly in aquatic organisms, is reviewed. Microplastics in the environment originate from a variety of sources. Quantitative information on the relevance of these sources is generally lacking, but first estimates indicate that abrasion and fragmentation of larger plastic items and materials containing synthetic polymers are likely to be most relevant. Microplastics are ingested and, mostly, excreted rapidly by numerous aquatic organisms. So far, there is no clear evidence of bioaccumulation or biomagnification. In laboratory studies, the ingestion of large amounts of microplastics mainly led to a lower food uptake and, consequently, reduced energy reserves and effects on other physiological functions. Based on the evaluated data, the lowest microplastic concentrations affecting marine organisms exposed via water are much higher than levels measured in marine water. In lugworms exposed via sediment, effects were observed at microplastic levels that were higher than those in subtidal sediments but in the same range as maximum levels in beach sediments. Hydrophobic contaminants are enriched on microplastics, but the available experimental results and modelling approaches indicate that the transfer of sorbed pollutants by microplastics is not likely to contribute significantly to bioaccumulation of these pollutants. Prior to being able to comprehensively assess possible environmental risks caused by microplastics a number of knowledge gaps need to be filled. However, in view of the persistence of microplastics in the environment, the high concentrations measured at some environmental sites and the prospective of strongly increasing concentrations, the release of plastics into the environment should be reduced in a broad and global effort regardless of a proof of an environmental risk.","Pesticides are indispensable in agricultural production. They have been used by farmers to control weeds and insects, and their remarkable increases in agricultural products have been reported. The increase in the world’s population in the 20th century could not have been possible without a parallel increase in food production. About one-third of agricultural products are produced depending on the application of pesticides. Without the use of pesticides, there would be a 78% loss of fruit production, a 54% loss of vegetable production, and a 32% loss of cereal production. Therefore, pesticides play a critical role in reducing diseases and increasing crop yields worldwide. Thus, it is essential to discuss the agricultural development process; the historical perspective, types and specific uses of pesticides; and pesticide behavior, its contamination, and adverse effects on the natural environment. The review study indicates that agricultural development has a long history in many places around the world. The history of pesticide use can be divided into three periods of time. Pesticides are classified by different classification terms such as chemical classes, functional groups, modes of action, and toxicity. Pesticides are used to kill pests and control weeds using chemical ingredients; hence, they can also be toxic to other organisms, including birds, fish, beneficial insects, and non-target plants, as well as air, water, soil, and crops. Moreover, pesticide contamination moves away from the target plants, resulting in environmental pollution. Such chemical residues impact human health through environmental and food contamination. In addition, climate change-related factors also impact on pesticide application and result in increased pesticide usage and pesticide pollution. Therefore, this review will provide the scientific information necessary for pesticide application and management in the future.","The rhizosphere is a hot spot of microbial interactions as exudates released by plant roots are a main food source for microorganisms and a driving force of their population density and activities. The rhizosphere harbors many organisms that have a neutral effect on the plant, but also attracts organisms that exert deleterious or beneficial effects on the plant. Microorganisms that adversely affect plant growth and health are the pathogenic fungi, oomycetes, bacteria and nematodes. Most of the soilborne pathogens are adapted to grow and survive in the bulk soil, but the rhizosphere is the playground and infection court where the pathogen establishes a parasitic relationship with the plant. The rhizosphere is also a battlefield where the complex rhizosphere community, both microflora and microfauna, interact with pathogens and influence the outcome of pathogen infection. A wide range of microorganisms are beneficial to the plant and include nitrogen-fixing bacteria, endo- and ectomycorrhizal fungi, and plant growth-promoting bacteria and fungi. This review focuses on the population dynamics and activity of soilborne pathogens and beneficial microorganisms. Specific attention is given to mechanisms involved in the tripartite interactions between beneficial microorganisms, pathogens and the plant. We also discuss how agricultural practices affect pathogen and antagonist populations and how these practices can be adopted to promote plant growth and health.","The industrialization of the agricultural sector has increased the chemical burden on natural ecosystems. Pesticides are agrochemicals used in agricultural lands, public health programs, and urban green areas in order to protect plants and humans from various diseases. However, due to their known ability to cause a large number of negative health and environmental effects, their side effects can be an important environmental health risk factor. The urgent need for a more sustainable and ecological approach has produced many innovative ideas, among them agriculture reforms and food production implementing sustainable practice evolving to food sovereignty. It is more obvious than ever that the society needs the implementation of a new agricultural concept regarding food production, which is safer for man and the environment, and to this end, steps such as the declaration of Nyéléni have been taken.","The different compartments of the gastrointestinal tract are inhabited by populations of micro-organisms. By far the most important predominant populations are in the colon where a true symbiosis with the host exists that is a key for well-being and health. For such a microbiota, ‘normobiosis’ characterises a composition of the gut ‘ecosystem’ in which micro-organisms with potential health benefits predominate in number over potentially harmful ones, in contrast to ‘dysbiosis’, in which one or a few potentially harmful micro-organisms are dominant, thus creating a disease-prone situation. The present document has been written by a group of both academic and industry experts (in the ILSI Europe Prebiotic Expert Group and Prebiotic Task Force, respectively). It does not aim to propose a new definition of a prebiotic nor to identify which food products are classified as prebiotic but rather to validate and expand the original idea of the prebiotic concept (that can be translated in ‘prebiotic effects’), defined as: ‘The selective stimulation of growth and/or activity(ies) of one or a limited number of microbial genus(era)/species in the gut microbiota that confer(s) health benefits to the host.’ Thanks to the methodological and fundamental research of microbiologists, immense progress has very recently been made in our understanding of the gut microbiota. A large number of human intervention studies have been performed that have demonstrated that dietary consumption of certain food products can result in statistically significant changes in the composition of the gut microbiota in line with the prebiotic concept. Thus the prebiotic effect is now a well-established scientific fact. The more data are accumulating, the more it will be recognised that such changes in the microbiota's composition, especially increase in bifidobacteria, can be regarded as a marker of intestinal health. The review is divided in chapters that cover the major areas of nutrition research where a prebiotic effect has tentatively been investigated for potential health benefits. The prebiotic effect has been shown to associate with modulation of biomarkers and activity(ies) of the immune system. Confirming the studies in adults, it has been demonstrated that, in infant nutrition, the prebiotic effect includes a significant change of gut microbiota composition, especially an increase of faecal concentrations of bifidobacteria. This concomitantly improves stool quality (pH, SCFA, frequency and consistency), reduces the risk of gastroenteritis and infections, improves general well-being and reduces the incidence of allergic symptoms such as atopic eczema. Changes in the gut microbiota composition are classically considered as one of the many factors involved in the pathogenesis of either inflammatory bowel disease or irritable bowel syndrome. The use of particular food products with a prebiotic effect has thus been tested in clinical trials with the objective to improve the clinical activity and well-being of patients with such disorders. Promising beneficial effects have been demonstrated in some preliminary studies, including changes in gut microbiota composition (especially increase in bifidobacteria concentration). Often associated with toxic load and/or miscellaneous risk factors, colon cancer is another pathology for which a possible role of gut microbiota composition has been hypothesised. Numerous experimental studies have reported reduction in incidence of tumours and cancers after feeding specific food products with a prebiotic effect. Some of these studies (including one human trial) have also reported that, in such conditions, gut microbiota composition was modified (especially due to increased concentration of bifidobacteria). Dietary intake of particular food products with a prebiotic effect has been shown, especially in adolescents, but also tentatively in postmenopausal women, to increase Ca absorption as well as bone Ca accretion and bone mineral density. Recent data, both from experimental models and from human studies, support the beneficial effects of particular food products with prebiotic properties on energy homaeostasis, satiety regulation and body weight gain. Together, with data in obese animals and patients, these studies support the hypothesis that gut microbiota composition (especially the number of bifidobacteria) may contribute to modulate metabolic processes associated with syndrome X, especially obesity and diabetes type 2. It is plausible, even though not exclusive, that these effects are linked to the microbiota-induced changes and it is feasible to conclude that their mechanisms fit into the prebiotic effect. However, the role of such changes in these health benefits remains to be definitively proven. As a result of the research activity that followed the publication of the prebiotic concept 15 years ago, it has become clear that products that cause a selective modification in the gut microbiota's composition and/or activity(ies) and thus strengthens normobiosis could either induce beneficial physiological effects in the colon and also in extra-intestinal compartments or contribute towards reducing the risk of dysbiosis and associated intestinal and systemic pathologies.",24.6394301501849,1,product
